https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py
This part shows diffrent method to create Tensor in Pytorch, and other operations for tensor.
本内容为创建tensor的方法和一些tensor的操作，如加、乘等

import torch
import numpy as np
############################
###        Part 1        ###
############################
data = [[1, 2], [3, 4]]
x_data = torch.tensor(data)
print(data) 
print(x_data)
# data is a list with several lists in it
# x_data has changed them into tensors
# data 列表
# x_data 张量

Output:
[[1, 2], [3, 4]]
tensor([[1, 2],
        [3, 4]])

############################
###        Part 2        ###
############################
np_array = np.array(data)
x_np = torch.from_numpy(np_array)
print(data)
print(np_array)
print(x_np)
print(type(data))
print(type(np_array))
print(x_np)
"""
It is quite clear that np_array change list into the type 'numpy.ndarray'
I met it once before when I had machine learning class last semester
x_np is clearly another way to creat Tensor(not from the original list) but from 'numpy.ndarray'(what has already been processed by the numpy package
从numpy.array创建张量，当然numpy.array是从list创建的
"""

Output:
[[1, 2], [3, 4]]
[[1 2]
 [3 4]]
tensor([[1, 2],
        [3, 4]])
<class 'list'>
<class 'numpy.ndarray'>
tensor([[1, 2],
        [3, 4]])
        
       
############################
###        Part 3        ###
############################       
x_ones = torch.ones_like(x_data) # retains the properties of x_data
print(x_data)
print(f"Ones Tensor: \n {x_ones} \n")

x_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data
print(f"Random Tensor: \n {x_rand} \n")

"""
It provide another method to creat Tensor
clearly the function torch.ones_like() return Tensor with the same size as x_data, and what in the tensor is all 0
https://pytorch.org/docs/stable/generated/torch.ones_like.html
x_data提供的是创建的张量的大小，与MATLAB中的ones差不了太多

torch.rand_like() function creat random elements.
https://pytorch.org/docs/stable/generated/torch.rand_like.html?highlight=torch%20rand_like#torch.rand_like
"""

Output:
tensor([[1, 2],
        [3, 4]])
Ones Tensor: 
 tensor([[1, 1],
        [1, 1]]) 

Random Tensor: 
 tensor([[0.9952, 0.0162],
        [0.4122, 0.8915]]) 


# To go further:
shape = (2, 3,)
rand_tensor = torch.rand(shape)
ones_tensor = torch.ones(shape)
zeros_tensor = torch.zeros(shape)

print(f"Random Tensor: \n {rand_tensor} \n")
print(f"Ones Tensor: \n {ones_tensor} \n")
print(f"Zeros Tensor: \n {zeros_tensor}")

Output:
Random Tensor: 
 tensor([[0.0181, 0.6123, 0.0709],
        [0.9727, 0.6047, 0.7873]]) 

Ones Tensor: 
 tensor([[1., 1., 1.],
        [1., 1., 1.]]) 

Zeros Tensor: 
 tensor([[0., 0., 0.],
        [0., 0., 0.]])
       
       

############################
###        Part 4        ###
############################  
# Tensor Attributes
# Tensor attributes describe their shape, datatype, and the device on which they are stored.
tensor = torch.rand(3, 4)

print(f"Shape of tensor: {tensor.shape}")
print(f"Datatype of tensor: {tensor.dtype}")
print(f"Device tensor is stored on: {tensor.device}")

Output:
Shape of tensor: torch.Size([3, 4])
Datatype of tensor: torch.float32
Device tensor is stored on: cpu

# Tensor Operations
# This can be test on Colab with GPU
# We move our tensor to the GPU if available
if torch.cuda.is_available():
  tensor = tensor.to('cuda')
  print(f"Device tensor is stored on: {tensor.device}")
  
Output:
Device tensor is stored on: cuda:0

############################
###        Part 5        ###
############################ 
# How to change the value
tensor = torch.ones(4, 4)
print(tensor)
tensor[:,1] = 0
print(tensor)
tensor[2,:] = 2
print(tensor)
tensor[1:] = 1
print(tensor)
tensor[:2,] = 4
print(tensor)

Output:
tensor([[1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.]])
tensor([[1., 0., 1., 1.],
        [1., 0., 1., 1.],
        [1., 0., 1., 1.],
        [1., 0., 1., 1.]])
tensor([[1., 0., 1., 1.],
        [1., 0., 1., 1.],
        [2., 2., 2., 2.],
        [1., 0., 1., 1.]])
tensor([[1., 0., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.]])
tensor([[4., 4., 4., 4.],
        [4., 4., 4., 4.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.]])
        
        
############################
###        Part 6        ###
############################  
# Joining tensors
t1 = torch.cat([tensor, tensor, tensor], dim=1)
print(t1)

"""
remember the tensor now is:
tensor([[4., 4., 4., 4.],
        [4., 4., 4., 4.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.]])
we can find it repeat three times
https://pytorch.org/docs/stable/generated/torch.cat.html?highlight=torch%20cat#torch.cat
"""

Output:
tensor([[4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.],
        [4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])


############################
###        Part 7        ###
############################  
# This computes the element-wise product
print(f"tensor.mul(tensor) \n {tensor.mul(tensor)} \n")
# Alternative syntax:
print(f"tensor * tensor \n {tensor * tensor}")

"""
I find it's quite similar as what we do in matlab(especially matrix)
https://pytorch.org/docs/stable/generated/torch.mul.html#torch.mul
"""

Output:
tensor.mul(tensor) 
 tensor([[16., 16., 16., 16.],
        [16., 16., 16., 16.],
        [ 1.,  1.,  1.,  1.],
        [ 1.,  1.,  1.,  1.]]) 

tensor * tensor 
 tensor([[16., 16., 16., 16.],
        [16., 16., 16., 16.],
        [ 1.,  1.,  1.,  1.],
        [ 1.,  1.,  1.,  1.]])

# This computes the matrix multiplication between two tensors
print(f"tensor.matmul(tensor.T) \n {tensor.matmul(tensor.T)} \n")
print(f"tensor.T \n {tensor.T} \n")
# Alternative syntax:
print(f"tensor @ tensor.T \n {tensor @ tensor.T}")

"""
Recall:
tensor:
tensor([[4., 4., 4., 4.],
        [4., 4., 4., 4.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.]])
tensor.T:
tensor([[4., 4., 1., 1.],
        [4., 4., 1., 1.],
        [4., 4., 1., 1.],
        [4., 4., 1., 1.]])
It is the same as what we do when compute matrix
"""


Output:
tensor.matmul(tensor.T) 
 tensor([[64., 64., 16., 16.],
        [64., 64., 16., 16.],
        [16., 16.,  4.,  4.],
        [16., 16.,  4.,  4.]]) 

tensor.T 
 tensor([[4., 4., 1., 1.],
        [4., 4., 1., 1.],
        [4., 4., 1., 1.],
        [4., 4., 1., 1.]]) 

tensor @ tensor.T 
 tensor([[64., 64., 16., 16.],
        [64., 64., 16., 16.],
        [16., 16.,  4.,  4.],
        [16., 16.,  4.,  4.]])


############################
###        Part 8        ###
############################  
print(tensor, "\n")
tensor.add_(5)
print(tensor)

"""
same as matlab
broadcasting
https://pytorch.org/docs/stable/generated/torch.add.html#torch.add
"""

Output:
tensor([[4., 4., 4., 4.],
        [4., 4., 4., 4.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.]]) 

tensor([[9., 9., 9., 9.],
        [9., 9., 9., 9.],
        [6., 6., 6., 6.],
        [6., 6., 6., 6.]])



############################
###        Part 9        ###
############################ 
# numpy array <-> tensor

## Tensor to NumPy array 
t = torch.ones(5)
print(f"t: {t}")
n = t.numpy()
print(f"n: {n}")

Output:
t: tensor([1., 1., 1., 1., 1.])
n: [1. 1. 1. 1. 1.]

## A change in the tensor reflects in the NumPy array.
t.add_(1)
print(f"t: {t}")
print(f"n: {n}")

Output:
t: tensor([2., 2., 2., 2., 2.])
n: [2. 2. 2. 2. 2.]


## NumPy array to Tensor
n = np.ones(5)
t = torch.from_numpy(n)
print(f"t: {t}")
print(f"n: {n}")


Output:
t: tensor([1., 1., 1., 1., 1.], dtype=torch.float64)
n: [1. 1. 1. 1. 1.]


## Changes in the NumPy array reflects in the tensor.
np.add(n, 1, out=n)
print(f"t: {t}")
print(f"n: {n}")

Output:
t: tensor([2., 2., 2., 2., 2.], dtype=torch.float64)
n: [2. 2. 2. 2. 2.]



________________________edit 2/7/2022_________________________
(Happy Birthday for me!)
